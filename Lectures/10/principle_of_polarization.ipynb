{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9352c96c-3afc-4e0d-883e-45454148d2e1",
   "metadata": {},
   "source": [
    "\n",
    "$\\begin{aligned} H(Y|X) &= -E \\log P(Y|X) \\\\ &= -\\sum_{x,y} P(x,y) \\log P(y|x) \\\\ &= -\\sum_{x,y} P(y|x) \\cdot P(x) \\log P(y|x) \\\\ &= - \\Big[ P(y=0|x=0) P(x=0) \\log P(y=0|x=0) + P(y=0|x=1) P(x=1) \\log P(y=0|x=1) + P(y=1|x=0) P(x=0) \\log P(y=1|x=0) + P(y=1|x=1) P(x=1) \\log P(y=1|x=1) \\Big] \\\\ &= - \\Big[ (1 - \\epsilon) q \\log(1 - \\epsilon) + \\epsilon( 1 - q) \\log \\epsilon + \\epsilon q \\log \\epsilon + (1 - \\epsilon)(1 - q) \\log (1 - \\epsilon) \\Big] \\\\ &= - \\Big[ \\Big( (1 - \\epsilon) q + (1 - \\epsilon) (1 - q) \\Big) \\log(1 - \\epsilon) + \\Big( \\epsilon( 1 - q) + \\epsilon q  \\Big) \\log \\epsilon \\Big] \\\\ &= - \\Big[ \\Big( \\not{q} - \\not{q\\epsilon} + 1 - \\not{\\epsilon} - \\epsilon + \\not{q\\epsilon} \\Big) \\log(1 - \\epsilon) + \\Big( \\epsilon \\log \\epsilon  \\Big) \\Big] \\\\ &= - \\Big[ ( 1 - \\epsilon ) \\log (1 - \\epsilon) + \\epsilon \\log \\epsilon \\Big] = H_2(\\epsilon) = H(y|x) \\end{aligned}$\n",
    "\n",
    "\n",
    "\n",
    "Here is the rewritten proof:\n",
    "\n",
    "\n",
    "$\\begin{aligned}\n",
    "H(Y|X) &= -E[\\log P(Y|X)] \\\\\n",
    "&= - \\sum P(x, y) \\log P(y|x) \\\\\n",
    "&= - \\sum P(y|x) P(x) \\log P(y|x) \\\\\n",
    "&= - \\big[ (1-\\epsilon)q \\log(1-\\epsilon) + \\epsilon(1-q) \\log \\epsilon + \\epsilon q \\log \\epsilon + (1-\\epsilon)(1-q) \\log(1-\\epsilon) \\big] \\\\\n",
    "&= - \\big[ ((1-\\epsilon)q + (1-\\epsilon)(1-q)) \\log(1-\\epsilon) + (\\epsilon(1-q) + \\epsilon q) \\log \\epsilon \\big] \\\\\n",
    "&= - \\big[ (1-\\epsilon) \\log(1-\\epsilon) + \\epsilon \\log \\epsilon \\big] = H_2(\\epsilon)\n",
    "\\end{aligned}\n",
    "$\n",
    "\n",
    "This shows that the conditional entropy $H(Y|X)$ for a Binary Symmetric Channel is equal to the binary entropy function $H_2(\\epsilon)$. Let me know if you'd like a more detailed explanation of any step!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29abf0d-6eb7-417c-bb55-24e195666175",
   "metadata": {},
   "source": [
    "\n",
    "This derivation computes the **conditional entropy** $H(Y|X)$ for a binary symmetric channel (BSC) and shows that it simplifies to the binary entropy function $H_2(\\epsilon)$, which characterizes the uncertainty introduced by the channel.\n",
    "\n",
    "### Step-by-Step Explanation:\n",
    "\n",
    "1. **Definition of Conditional Entropy**:\n",
    "$H(Y|X) = -E[\\log P(Y|X)].$\n",
    "   This measures the uncertainty in $Y$ given $X$, weighted by the joint distribution $P(X, Y)$.\n",
    "\n",
    "2. **Rewriting with Joint Probabilities**:\n",
    "   The expectation can be expanded as:\n",
    "$H(Y|X) = -\\sum_{x, y} P(x, y) \\log P(y|x),$\n",
    "   which decomposes into terms involving $P(x)$ and $P(y|x)$:\n",
    "$H(Y|X) = -\\sum_{x, y} P(y|x) \\cdot P(x) \\log P(y|x).$\n",
    "\n",
    "3. **Binary Symmetric Channel (BSC) Setup**:\n",
    "   For a BSC with crossover probability $\\epsilon$, the transition probabilities are:\n",
    "   - $P(Y = 0 | X = 0) = 1 - \\epsilon$,\n",
    "   - $P(Y = 1 | X = 0) = \\epsilon$,\n",
    "   - $P(Y = 1 | X = 1) = 1 - \\epsilon$,\n",
    "   - $P(Y = 0 | X = 1) = \\epsilon$.\n",
    "\n",
    "   The probabilities $P(X = 0) = q$ and $P(X = 1) = 1 - q$ describe the input distribution.\n",
    "\n",
    "4. **Breaking Down the Terms**:\n",
    "   Substitute the transition probabilities into the summation:\n",
    "$H(Y|X) = - \\Big[ P(y = 0 | x = 0) P(x = 0) \\log P(y = 0 | x = 0) + P(y = 0 | x = 1) P(x = 1) \\log P(y = 0 | x = 1) + \\dots \\Big].$\n",
    "   Explicitly:\n",
    "$H(Y|X) = - \\Big[ (1 - \\epsilon)q \\log(1 - \\epsilon) + \\epsilon (1 - q) \\log \\epsilon + \\epsilon q \\log \\epsilon + (1 - \\epsilon)(1 - q) \\log(1 - \\epsilon) \\Big].$\n",
    "\n",
    "5. **Grouping Terms**:\n",
    "   Group terms with common factors:\n",
    "$H(Y|X) = - \\Big[ \\big((1 - \\epsilon)q + (1 - \\epsilon)(1 - q)\\big) \\log(1 - \\epsilon) + \\big(\\epsilon q + \\epsilon(1 - q)\\big) \\log \\epsilon \\Big].$\n",
    "   Simplify the coefficients:\n",
    "   - $(1 - \\epsilon)q + (1 - \\epsilon)(1 - q) = 1 - \\epsilon$,\n",
    "   - $\\epsilon q + \\epsilon(1 - q) = \\epsilon$.\n",
    "\n",
    "   Thus:\n",
    "$H(Y|X) = - \\Big[ (1 - \\epsilon) \\log(1 - \\epsilon) + \\epsilon \\log \\epsilon \\Big].$\n",
    "\n",
    "6. **Binary Entropy Function**:\n",
    "   The result matches the binary entropy function $H_2(\\epsilon)$, which is defined as:\n",
    "$H_2(\\epsilon) = -\\epsilon \\log \\epsilon - (1 - \\epsilon) \\log(1 - \\epsilon).$\n",
    "   Therefore:\n",
    "$H(Y|X) = H_2(\\epsilon).$\n",
    "\n",
    "### Interpretation:\n",
    "- $H(Y|X)$ measures the uncertainty introduced by the channel's noise. For a BSC, this uncertainty depends only on the crossover probability $\\epsilon$ and is independent of the input distribution $P(X)$.\n",
    "- The result $H_2(\\epsilon)$ shows that the conditional entropy is maximal when $\\epsilon = 0.5$ (the most uncertain case) and minimal when $\\epsilon = 0$ or $\\epsilon = 1$ (no uncertainty)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c24c5d-219a-40fd-9ee4-aa5225ef90fd",
   "metadata": {},
   "source": [
    "\n",
    "$P(y = 1) = P(y = 1 | x = 0)P(x = 0) + P(y = 1 | x = 1)P(x = 1),$\n",
    "\n",
    "$= \\epsilon q + (1 - \\epsilon)(1 - q).$\n",
    "\n",
    "$H(Y) = - \\Big[ [(1 - \\epsilon)q + \\epsilon(1 - q)] \\log [(1 - \\epsilon)q + \\epsilon(1 - q)] + [\\epsilon q + (1 - \\epsilon)(1 - q)] \\log [\\epsilon q + (1 - \\epsilon)(1 - q)] \\Big].$\n",
    "\n",
    "$I(X; Y) = H(Y) - H_2(\\epsilon).$\n",
    "\n",
    "$H_2(\\epsilon) \\quad \\text{does not affect optimization over } P(x), \\text{ i.e., over } q.$\n",
    "\n",
    "$H(Y) \\text{ is maximized when } P(y = 0) = P(y = 1) = \\frac{1}{2}.$\n",
    "\n",
    "$(1 - \\epsilon)q + \\epsilon(1 - q) = \\epsilon q + (1 - \\epsilon)(1 - q) = \\frac{1}{2}.$\n",
    "\n",
    "$(1 - \\epsilon)q + \\epsilon - \\epsilon q = \\epsilon q + (1 - \\epsilon) - q(1 - \\epsilon).$\n",
    "\n",
    "$q(1 - \\epsilon) + \\epsilon - \\epsilon q = \\epsilon q + 1 - \\epsilon - q + q\\epsilon.$\n",
    "\n",
    "$q(1 - \\epsilon) + \\epsilon = 1 - \\epsilon - q.$\n",
    "\n",
    "$q(1 - \\epsilon) + q = 1 - \\epsilon.$\n",
    "\n",
    "$q(1 + (1 - \\epsilon)) = 1 - \\epsilon.$\n",
    "\n",
    "$q = \\frac{1}{2}.$\n",
    "\n",
    "$H(Y) = 1.$\n",
    "\n",
    "$X \\sim P_X, \\quad \\mathcal{X} = \\{0, 1\\}$\n",
    "\n",
    "$\\text{Let } X_1, X_2, \\dots, \\quad \\text{i.i.d. } \\sim X.$\n",
    "\n",
    "$\\text{Let } \\mathcal{N} = 2^n, \\quad n > 1.$\n",
    "\n",
    "$\\text{Let } X^n = [X_1, X_2, \\dots, X_n].$\n",
    "\n",
    "$X^i = [X_1, X_2, \\dots, X_i].$\n",
    "\n",
    "$F = \\begin{bmatrix} 1 & 0 \\\\ 1 & 1 \\end{bmatrix} \\quad \\in \\mathbb{F}_2.$\n",
    "\n",
    "$F_{N \\times N}^{\\otimes n} = n\\text{-Kronecker product.}$\n",
    "\n",
    "$F^{\\otimes 2} = \\begin{pmatrix} 1 & 0 & 0 & 0 \\\\ 1 & 1 & 0 & 0 \\\\ 1 & 0 & 1 & 0 \\\\ 1 & 1 & 1 & 1 \\end{pmatrix}$$F^{\\otimes 3} = \n",
    "\\begin{pmatrix} \n",
    "\\begin{array}{c|c} \n",
    "\\begin{matrix} 1 & 0 & 0 & 0 \\\\ 1 & 1 & 0 & 0 \\\\ 1 & 0 & 1 & 0 \\\\ 1 & 1 & 1 & 1 \\end{matrix} &\n",
    "\\begin{matrix} 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\end{matrix} \\\\\n",
    "\\hline F^{\\otimes 2} & F^{\\otimes 3} \n",
    "\\end{array} \n",
    "\\end{pmatrix}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae4207a-2393-443e-a703-511cb30d9533",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.1",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
