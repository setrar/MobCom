{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "5c45b8ab-64df-4de8-9024-4dc34e373739",
      "metadata": {},
      "source": [
        "# 2 Proofs Fundamental to Info Theo"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d0a20d4-f14c-4b37-bb1f-5444cb296ca9",
      "metadata": {},
      "source": [
        "## 1) Proof Capacity of Binary Erasure Channel (BEC) "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ea63b8f-f723-4870-b477-6d3e1bfce669",
      "metadata": {},
      "source": [
        "![](images/Fig1_BEC.png)\n",
        "\n",
        "$P(Y|X)$\n",
        "\n",
        "$P(x=0)=2 \\quad P(x=1)=1-q$\n",
        "\n",
        "$C = \\max_{P(X)} I(X; Y) = \\max_{q \\in [0,1]} I(X; Y).$\n",
        "\n",
        "$I(X; Y) = H(Y) - H(Y|X)$\n",
        "\n",
        "$P(y=0|x=0)= 1 - \\epsilon$\n",
        "\n",
        "$P(y=1|x=0)= \\epsilon$\n",
        "\n",
        "$P(y=0|x=1)= \\epsilon$\n",
        "\n",
        "$P(y=1|x=1)= 1 - \\epsilon$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8700b867-fdd7-46fb-bb57-1793ca4c28b9",
      "metadata": {},
      "source": [
        "## Proof"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a42698ef-8a9d-4d80-bc64-b8085e268b28",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "0f30f7c5-97d1-4468-aa49-bc1060d753cc",
      "metadata": {},
      "source": [
        "# References"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bfcad89c-5beb-4aee-873d-62c278f802a1",
      "metadata": {},
      "source": [
        "### Proof of the Capacity of the Binary Erasure Channel (BEC)\n",
        "\n",
        "The Binary Erasure Channel (BEC) is characterized by the following behavior:\n",
        "- The input $X \\in \\{0, 1\\}$ is transmitted.\n",
        "- The output $Y \\in \\{0, 1, e\\}$, where $e$ indicates an erasure.\n",
        "- The probability of erasure is $\\epsilon$, and the probability of correctly receiving the input bit is $1 - \\epsilon$.\n",
        "\n",
        "The goal is to determine the **channel capacity**, which is the maximum mutual information $I(X; Y)$ between the input $X$ and the output $Y$.\n",
        "\n",
        "---\n",
        "\n",
        "### Step 1: Definition of Mutual Information\n",
        "The mutual information is:\n",
        "$I(X; Y) = H(X) - H(X|Y),$\n",
        "where:\n",
        "- $H(X)$ is the entropy of the input,\n",
        "- $H(X|Y)$ is the conditional entropy of $X$ given $Y$.\n",
        "\n",
        "---\n",
        "\n",
        "### Step 2: Entropy $H(X)$\n",
        "Let the input distribution be $P(X = 0) = q$ and $P(X = 1) = 1 - q$. The entropy of the input is:\n",
        "$H(X) = - \\big[ q \\log_2 q + (1 - q) \\log_2 (1 - q) \\big].$\n",
        "To maximize $I(X; Y)$, the input distribution should maximize $H(X)$. The maximum entropy occurs when $q = 0.5$, so:\n",
        "$H(X) = 1.$\n",
        "\n",
        "---\n",
        "\n",
        "### Step 3: Conditional Entropy $H(X|Y)$\n",
        "The conditional entropy $H(X|Y)$ depends on the behavior of the channel. The output $Y$ can take three possible values ($0, 1, e$):\n",
        "- If $Y = 0$, then $X = 0$ with certainty.\n",
        "- If $Y = 1$, then $X = 1$ with certainty.\n",
        "- If $Y = e$, no information about $X$ is gained, and $H(X|Y = e) = H(X)$.\n",
        "\n",
        "The total conditional entropy is a weighted sum over these cases:\n",
        "$H(X|Y) = P(Y = 0) H(X|Y = 0) + P(Y = 1) H(X|Y = 1) + P(Y = e) H(X|Y = e).$\n",
        "- $H(X|Y = 0) = 0$ (certainty about $X$),\n",
        "- $H(X|Y = 1) = 0$ (certainty about $X$),\n",
        "- $H(X|Y = e) = H(X) = 1$ (no information about $X$).\n",
        "\n",
        "The probabilities of $Y = 0, Y = 1,$ and $Y = e$ are:\n",
        "$P(Y = 0) = \\frac{1}{2}(1-\\epsilon), \\quad P(Y = 1) = \\frac{1}{2}(1-\\epsilon), \\quad P(Y = e) = \\epsilon.$\n",
        "\n",
        "Substitute into the formula:\n",
        "$H(X|Y) = \\frac{1}{2}(1-\\epsilon)(0) + \\frac{1}{2}(1-\\epsilon)(0) + \\epsilon(1) = \\epsilon.$\n",
        "\n",
        "---\n",
        "\n",
        "### Step 4: Compute Mutual Information\n",
        "The mutual information is:\n",
        "$I(X; Y) = H(X) - H(X|Y).$\n",
        "Substitute $H(X) = 1$ and $H(X|Y) = \\epsilon$:\n",
        "$I(X; Y) = 1 - \\epsilon.$\n",
        "\n",
        "---\n",
        "\n",
        "### Step 5: Maximize Mutual Information\n",
        "The channel capacity is the maximum mutual information $C = \\max_{P(X)} I(X; Y)$. Since $I(X; Y)$ is maximized when $P(X = 0) = P(X = 1) = 0.5$, the capacity is:\n",
        "$C = 1 - \\epsilon.$\n",
        "\n",
        "---\n",
        "\n",
        "### Final Result\n",
        "The capacity of the Binary Erasure Channel is:\n",
        "$C = 1 - \\epsilon,$\n",
        "where $\\epsilon$ is the erasure probability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fbf6721b-d3d8-4b24-a092-df1d7033061b",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Julia 1.11.1",
      "language": "julia",
      "name": "julia-1.11"
    },
    "language_info": {
      "file_extension": ".jl",
      "mimetype": "application/julia",
      "name": "julia",
      "version": "1.11.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}