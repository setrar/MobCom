{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c45b8ab-64df-4de8-9024-4dc34e373739",
   "metadata": {},
   "source": [
    "# 2 Proofs Fundamental to Info Theo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0a20d4-f14c-4b37-bb1f-5444cb296ca9",
   "metadata": {},
   "source": [
    "## 1) Proof Capacity of Binary Symmetric Channel (BSC) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea63b8f-f723-4870-b477-6d3e1bfce669",
   "metadata": {},
   "source": [
    "![](images/Fig1_BSC.png)\n",
    "\n",
    "$P(Y|X)$\n",
    "\n",
    "$P(x=0)=2 \\quad P(x=1)=1-q$\n",
    "\n",
    "$C = \\max_{P(X)} I(X; Y) = \\max_{q \\in [0,1]} I(X; Y).$\n",
    "\n",
    "$I(X; Y) = H(Y) - H(Y|X)$\n",
    "\n",
    "$P(y=0|x=0)= 1 - \\epsilon$\n",
    "\n",
    "$P(y=1|x=0)= \\epsilon$\n",
    "\n",
    "$P(y=0|x=1)= \\epsilon$\n",
    "\n",
    "$P(y=1|x=1)= 1 - \\epsilon$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8700b867-fdd7-46fb-bb57-1793ca4c28b9",
   "metadata": {},
   "source": [
    "## Proof"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b26aa53-9b9c-467f-9adb-e711aee9dc4e",
   "metadata": {},
   "source": [
    "Here is the transcription of the text from the image:\n",
    "\n",
    "---\n",
    "\n",
    "$H(Y|X) = -E \\log P(Y|X)$\n",
    "\n",
    "$= -\\sum_{x, y} P(x, y) \\log P(y|x)$\n",
    "\n",
    "$= -\\sum_{x, y} P(y|x) P(x) \\log P(y|x)$\n",
    "\n",
    "$= -\\Big[ P(y = 0 | x = 0) P(x = 0) \\log P(y = 0 | x = 0) + P(y = 0 | x = 1) P(x = 1) \\log P(y = 0 | x = 1)$\n",
    "\n",
    "$+ P(y = 1 | x = 0) P(x = 0) \\log P(y = 1 | x = 0) + P(y = 1 | x = 1) P(x = 1) \\log P(y = 1 | x = 1) \\Big].$\n",
    "\n",
    "$= -\\Big[ (1 - \\epsilon)q \\log(1 - \\epsilon) + \\epsilon q \\log \\epsilon + \\epsilon (1 - q) \\log \\epsilon + (1 - \\epsilon)(1 - q) \\log(1 - \\epsilon) \\Big].$\n",
    "\n",
    "$= -\\Big[ (1 - \\epsilon)(q + 1 - q) \\log (1 - \\epsilon) + (\\epsilon q + \\epsilon (1 - q)) \\log \\epsilon \\Big].$\n",
    "\n",
    "$= -\\Big[ (1 - \\epsilon) \\log (1 - \\epsilon) + \\epsilon \\log \\epsilon \\Big].$\n",
    "\n",
    "$= H_2(\\epsilon) = H(Y|X).$\n",
    "\n",
    "$H(Y) = -E \\log P(y) = -\\sum_y P(y) \\log(P(y)).$\n",
    "\n",
    "$= - \\Big[ P(y = 0) \\log P(y = 0) + P(y = 1) \\log P(y = 1) \\Big].$\n",
    "\n",
    "$P(y = 0) = P(y = 0 | x = 0)P(x = 0) + P(y = 0 | x = 1)P(x = 1),$\n",
    "\n",
    "$= (1 - \\epsilon)q + \\epsilon(1 - q).$\n",
    "\n",
    "$P(y = 1) = P(y = 1 | x = 0)P(x = 0) + P(y = 1 | x = 1)P(x = 1),$\n",
    "\n",
    "$= \\epsilon q + (1 - \\epsilon)(1 - q).$\n",
    "\n",
    "$H(Y) = - \\Big[ [(1 - \\epsilon)q + \\epsilon(1 - q)] \\log [(1 - \\epsilon)q + \\epsilon(1 - q)] + [\\epsilon q + (1 - \\epsilon)(1 - q)] \\log [\\epsilon q + (1 - \\epsilon)(1 - q)] \\Big].$\n",
    "\n",
    "$I(X; Y) = H(Y) - H_2(\\epsilon).$\n",
    "\n",
    "$H_2(\\epsilon) \\quad \\text{does not affect optimization over } P(x), \\text{ i.e., over } q.$\n",
    "\n",
    "$H(Y) \\text{ is maximized when } P(y = 0) = P(y = 1) = \\frac{1}{2}.$\n",
    "\n",
    "$(1 - \\epsilon)q + \\epsilon(1 - q) = \\epsilon q + (1 - \\epsilon)(1 - q) = \\frac{1}{2}.$\n",
    "\n",
    "$(1 - \\epsilon)q + \\epsilon - \\epsilon q = \\epsilon q + (1 - \\epsilon) - q(1 - \\epsilon).$\n",
    "\n",
    "$q(1 - \\epsilon) + \\epsilon - \\epsilon q = \\epsilon q + 1 - \\epsilon - q + q\\epsilon.$\n",
    "\n",
    "$q(1 - \\epsilon) + \\epsilon = 1 - \\epsilon - q.$\n",
    "\n",
    "$q(1 - \\epsilon) + q = 1 - \\epsilon.$\n",
    "\n",
    "$q(1 + (1 - \\epsilon)) = 1 - \\epsilon.$\n",
    "\n",
    "$q = \\frac{1}{2}.$\n",
    "\n",
    "$H(Y) = 1.$\n",
    "\n",
    "---\n",
    "\n",
    "Let me know if you need further assistance!\n",
    "\n",
    "--- \n",
    "\n",
    "Let me know if you need further assistance!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f4b370-cabe-4cb4-93d9-a76c8d057403",
   "metadata": {},
   "source": [
    "$\\begin{gather} C_{\\text{BSC}} &= \\max_{q} I(X; Y) \\\\ &= H(Y) - H(\\epsilon) \\end{gather}$\n",
    "\n",
    "one concave and one convex graph was shown here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f30f7c5-97d1-4468-aa49-bc1060d753cc",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcad89c-5beb-4aee-873d-62c278f802a1",
   "metadata": {},
   "source": [
    "\n",
    "The Binary Symmetric Channel (BSC) is a channel where a binary input $X \\in \\{0,1\\}$ is transmitted, and the received output $Y \\in \\{0,1\\}$ may differ from $X$ with a probability $p$, known as the crossover probability. The goal is to find the capacity of this channel, which is the maximum achievable mutual information between the input $X$ and the output $Y$.\n",
    "\n",
    "### Channel Description\n",
    "The transition probabilities of the BSC are:\n",
    "$P(Y = y | X = x) = \\begin{cases} 1 - p & \\text{if } y = x, \\\\ p & \\text{if } y \\neq x. \\end{cases}$\n",
    "\n",
    "### Mutual Information $I(X; Y)$\n",
    "The mutual information between $X$ and $Y$ is:\n",
    "$I(X; Y) = H(Y) - H(Y|X),$\n",
    "where:\n",
    "- $H(Y)$ is the entropy of the output,\n",
    "- $H(Y|X)$ is the conditional entropy given the input.\n",
    "\n",
    "#### Step 1: Calculate $H(Y|X)$\n",
    "Since $H(Y|X)$ depends only on the channel transition probabilities:\n",
    "$H(Y|X) = - \\sum_{x \\in \\{0,1\\}} P(X = x) \\sum_{y \\in \\{0,1\\}} P(Y = y | X = x) \\log_2 P(Y = y | X = x).$\n",
    "Substituting $P(Y = y | X = x)$:\n",
    "$H(Y|X) = - \\left[ (1 - p) \\log_2(1 - p) + p \\log_2(p) \\right].$\n",
    "This term does not depend on the input distribution.\n",
    "\n",
    "#### Step 2: Calculate $H(Y)$\n",
    "The entropy $H(Y)$ of the output depends on the distribution of $Y$. Let the input probabilities be $P(X = 0) = q$ and $P(X = 1) = 1 - q$. Then, the probability distribution of $Y$ is:\n",
    "$P(Y = 0) = q(1-p) + (1-q)p,$\n",
    "$P(Y = 1) = q p + (1-q)(1-p).$\n",
    "Let $P(Y = 0) = r$. Then:\n",
    "$H(Y) = - \\left[ r \\log_2 r + (1 - r) \\log_2 (1 - r) \\right].$\n",
    "\n",
    "#### Step 3: Maximize $I(X; Y)$\n",
    "The mutual information is:\n",
    "$I(X; Y) = H(Y) - H(Y|X),$\n",
    "where $H(Y|X)$ is constant. To maximize $I(X; Y)$, we need to maximize $H(Y)$, which occurs when $r$ is maximized.\n",
    "\n",
    "From the symmetry of the channel, $H(Y)$ is maximized when the input distribution is uniform, i.e., $P(X = 0) = P(X = 1) = \\frac{1}{2}$. Under this condition:\n",
    "$r = \\frac{1}{2}(1-p) + \\frac{1}{2}p = \\frac{1}{2}.$\n",
    "Substituting $r = \\frac{1}{2}$ into $H(Y)$, we get:\n",
    "$H(Y) = 1.$\n",
    "\n",
    "#### Step 4: Compute Channel Capacity\n",
    "Finally, the channel capacity is:\n",
    "$C = \\max_{P(X)} I(X; Y).$\n",
    "For $P(X = 0) = P(X = 1) = \\frac{1}{2}$:\n",
    "$C = H(Y) - H(Y|X) = 1 - \\left[ (1-p) \\log_2(1-p) + p \\log_2(p) \\right].$\n",
    "\n",
    "### Final Result\n",
    "The capacity of the Binary Symmetric Channel is:\n",
    "$C = 1 - H(p),$\n",
    "where $H(p) = -p \\log_2 p - (1-p) \\log_2 (1-p)$ is the binary entropy function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4417bf10-4e15-4639-af06-02c248ff5fea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.1",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
